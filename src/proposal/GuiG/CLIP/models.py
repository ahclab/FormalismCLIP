import torch 
from torch import nn 
from transformers import CLIPVisionModelWithProjection, CLIPTokenizer, CLIPTextModelWithProjection
import sys 
from os.path import abspath, join, dirname 

parent_dir = abspath(join(dirname(__file__), '..')) 
sys.path.insert(0, parent_dir)

from Core.Models import SynTrfEncoder 
from nltk import Tree
import nltk  
import re
 


def POS_extractor(tree: nltk.Tree) -> list[str]:
    """ Extract POS tags from a tree """
    pos_tags_match = re.findall(r'\((\w+)', tree) 
    pos_tags = [tag for tag in pos_tags_match if tag.isalpha()] 
    
    return pos_tags 


def POS_extractora(tree: nltk.Tree) -> list[str]:
    """ Extract POS tags from a tree """
    pos_tags_match = re.findall(r'\(([\w\d_]+)', tree) 
    pos_tags = [tag for tag in pos_tags_match] 
    
    return pos_tags 


def POS_for_inputs(tree: nltk.Tree, dictionary: dict[str, int]) -> list[int]:
    """
    Get POS from the text, put <s> and </s>, and convert to tensor  
    """
    pos_tags_match = re.findall(r'\((\w+)', tree) 
    pos_tags = [tag for tag in pos_tags_match if tag.isalpha()]
    new_pos_tags = ['<s>'] + pos_tags + ['</s>'] 
    
    # convert 
    new_pos_tags = [dictionary[tag] if tag in dictionary.keys() else dictionary['<unk>'] for tag in new_pos_tags]
    # pad to length 51 with 0
    new_pos_tags += [0] * (51 - len(new_pos_tags))
    
    return new_pos_tags    


def number_tagger(tree:str) -> str:
    """
    To add a number to each tag in a tree for paths mask generation
    """
    new_str = [] 
    words = tree.split()  
    tags = {word : 0 for word in words if word.startswith('(')}
    
    for word in words:
        if word.startswith('('):
            tags[word] += 1 
            new_str.append(word + "_" + str(tags[word]))
        else:
            new_str.append(word)
    
    return ' '.join(new_str)  

            
def get_paths_to_leaves(tree):
    paths = []

    def traverse(node, current_path):
        current_path.append(node.label() if isinstance(node, nltk.Tree) else node)
        
        if isinstance(node, nltk.Tree):
            for child in node:
                traverse(child, current_path.copy())
        else:
            paths.append(tuple(current_path))

    traverse(tree, [])
    return paths


def path_index_generator(paths : list[tuple[str]], POS_list : list[str]) -> list[list[bool]]:
     
    whole_path_index = []
    for path in paths:
        path_index = []
        for POS in POS_list:
            if POS in path:
                path_index.append(True)
            else:
                path_index.append(False)
                
        # add False at the head and the tail of the path_index
        path_index.insert(0, False)
        path_index.append(False)
        # pad it to length 51 
        path_index += [False] * (51 - len(path_index)) 
        whole_path_index.append(path_index)
    
    # pad it to length 30 
    whole_path_index += [[False] * 51] * (30 - len(whole_path_index))
    
    return whole_path_index

def calculate_distance_to_root(tree, node):
    """
    Calculate the distance from a node to the root of the tree using a recursive approach.
    """
    if node == tree:
        return 0  # The root node has distance 0

    # Find the parent of the current node
    parent = None
    for subtree in tree.subtrees():
        if node in subtree:
            parent = subtree
            break

    # Recursively calculate the distance from the parent to the root
    return 1 + calculate_distance_to_root(tree, parent)

def level_sequence_for_input(tree: nltk.Tree, dictionary: dict[str, int]) -> list[int]:
    lev = [str(1 + calculate_distance_to_root(tree, node)) for node in tree.subtrees()] 
    # add <s> and </s>
    lev = ['<s>'] + lev + ['</s>']
    # convert 
    lev = [dictionary[level] if level in dictionary.keys() else dictionary['<unk>'] for level in lev]
    # pad to length 51 with 0 
    lev += [0] * (51 - len(lev))
    
    return lev 
    
    
def position_generator(pos: list[str]) -> list[int]: 
    """
    pos is the one generated by POS_extractor(tree)
    so no <s> and </s> in it 
    padding neither 
    """
    leng = len(pos) + 2 
    position = [i for i in range(1, leng + 1)] 
    # padding to length 51 
    position += [0] * (51 - len(position))
    
    return position 


def contrastive_loss(logits: torch.Tensor) -> torch.Tensor: 
    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device = logits.device)) 

def cliploss(sim: torch.Tensor) -> torch.Tensor: 
    graph_loss = contrastive_loss(sim) 
    image_loss = contrastive_loss(sim.t()) 
    return (graph_loss + image_loss) / 2.0  



class GuiGCLIP(nn.Module): 
    def __init__(self, device): 
        super().__init__() 
        self.device = device 
        self.tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained("openai/clip-vit-base-patch32").to(device)
        self.text_encoder = CLIPTextModelWithProjection.from_pretrained("openai/clip-vit-base-patch32").to(device)
        self.syn_token_emb = nn.Embedding(
            74, 512, padding_idx=0
        )
        self.lvl_token_emb = nn.Embedding(
            12, 512, padding_idx=0
        ) 
        self.syn_encoder = SynTrfEncoder(
            token_emb=self.syn_token_emb,
            lvl_emb=self.lvl_token_emb,
            max_seq_len=52,
            d_model=512,
            d_inner=1024,
            n_layer=6,
            n_head=8,
            d_k=64,
            d_v=64,
            dropout=0.1,
        ).to(device)
        
        self.dimension = 512 
        self.logit_scale = nn.Parameter(torch.tensor(2.6592)) 
        self.concater = nn.Linear(1024, 512) 
        
        
    def forward(
        self, 
        texts: list[str],
        syn_seqs: torch.LongTensor,
        lvl_seqs: torch.LongTensor,
        pos_seqs: torch.LongTensor,
        path_masks: torch.BoolTensor, 
        pixel_values: torch.FloatTensor, 
        return_loss: bool = True,
        output_attentions: bool = None,
        output_hidden_states: bool = None,
    ):    
        output_attentions = None
        output_hidden_states = None
        return_dict = None
        
        # encode the image
        vision_outputs = self.image_encoder(
            pixel_values = pixel_values,
            output_attentions = output_attentions,
            output_hidden_states = output_hidden_states,
            return_dict = return_dict,
        )
        vision_embeds = vision_outputs.image_embeds
        
        syn_embeds, syn_attn = self.syn_encoder(
            syn_seq = syn_seqs, 
            lvl_seq = lvl_seqs, 
            pos_seq = pos_seqs, 
            path_mask = path_masks
        )
        
        syn_embeds = torch.mean(syn_embeds, dim = 1) 
        
        
        text_inputs = self.tokenizer(texts, padding = True, return_tensors = "pt").to(self.device)
        text_outputs = self.text_encoder(**text_inputs) 
        text_embeds = text_outputs.text_embeds 
        
        
        # Normalisation
        vision_embeds = vision_embeds / vision_embeds.norm(p=2, dim=-1, keepdim=True)
        syn_embeds = syn_embeds / syn_embeds.norm(p=2, dim=-1, keepdim=True)
        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)
        
        concated = [] 
        
        for i in range(len(syn_embeds)): 
            concated.append(torch.cat((syn_embeds[i], text_embeds[i]), dim = 0)) 
        concated = torch.stack(concated, dim = 0) 
        concated = self.concater(concated) 
        
        
        
        # Cos similarity
        logit_scale = self.logit_scale.exp()
        logits_per_text = torch.matmul(concated, vision_embeds.t()) * logit_scale 
        logits_per_image = logits_per_text.t() 
        
        loss = None 
        if return_loss: 
            loss = cliploss(logits_per_text) 
            
        res = {
            'loss': loss, 
            'logits_per_text': logits_per_text,
            'logits_per_image': logits_per_image,
            'text_embeds': text_embeds,
            'image_embeds': vision_embeds,
            'syn_embeds': syn_embeds
        }
        
        
        
        return res 
    
    
    
        
        